NAME: Richard Khillah
EMAIL: RKhillah@ucla.edu
ID: 604853262

Limitations ... everything runs completely and all tests in the sanity check pass except for 2, which is really the result of failing one test, namely.

Source code descriptions:
Makefile ... builds the deliverable programs (lab2_add and lab2_list), output, graphs, and tarball using `make`, `make tests`, `make graphs`, and `make dist`, respectively.

lab2_add.c ... a C program that tests a shared variable add function, generates performance statistics, and takes the following command-line options:
	--threads=N
	--iterations=N
	--yield
	--sync=[m, c, s]

SortedList.h ... The header file for the sorted list interface

SortedList.c ... The implementation of the sorted list

utils.h ... the header file for standard commonly used utilies
utils.c ... the implementation of utils.h

helper.h ... the file defining variable common between lab2_list.c

lab2_list-helper.c ... implementing each how they use the functions in helper.h

lab2_list.c ... a C program that tests a shared linked-list using the methods described in the SortedList.h header file, generates performance statistics, and takes the following command-line options:
	--threads=N
	--iterations=N
	--yield=[i, d, l] (in any combination, e.g., ild)
	--sync=[m, s]

generate.sh ... script used to generate test cases

lab2_list.gp ... provided test scripts that use the below mentioned .csv files to generate the graphs for the lab.

lab2_list-?.png ... the required output image files produced by lab2_list.gp

lab2b_list.csv ... the file containing outputs for lab2b_list test cases in the Makefile. Also required file for running the above .gp test scripts


QUESTION 2.3.1 - CPU time in the basic list implementation:
Where do you believe most of the CPU time is spent in the 1 and 2-thread list tests ?
Why do you believe these to be the most expensive parts of the code?
Where do you believe most of the CPU time is being spent in the high-thread spin-lock tests?
Where do you believe most of the CPU time is being spent in the high-thread mutex tests?

Most of the CPU time during the 1- and 2-thread list tests is being spent actually performing list operations.

for the high-thread spin-lock tests, most of the CPU time is likely being spent spinning/waiting for a lock to be free.
for the high-thread mutex tests, most of the CPU time is likely being spent performing list operations since the thread will be sleeping until it aquires the lock.


QUESTION 2.3.2 - Execution Profiling:
Where (what lines of code) are consuming most of the CPU time when the spin-lock version of the list exerciser is run with a large number of threads?

Why does this operation become so expensive with large numbers of threads?




QUESTION 2.3.3 - Mutex Wait Time:
Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
Why does the average lock-wait time rise so dramatically with the number of contending threads?

Why does the completion time per operation rise (less dramatically) with the number of contending threads?

How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?

The average lock-wait time dramatically rises with the number of contending threads because there is more and more contension on a single lock, i.e., all the threads are waiting to get one lock. Completion time rises less dramatically because the time list operations take remains constant and the completion time averages total operation time.


QUESTION 2.3.4 - Performance of Partitioned Lists
Explain the change in performance of the synchronized methods as a function of the number of lists.
Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.

It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.

Throughput should continue to increase as the number of lengths increases, but not indefinatley, that is, afterall, the point. The multi-list essentially acts as a divide and conquer mechanism: splitting one list into many pieces, essentially distributing unique critical sections across a threadpool. 

In this scenario, the more threads there are the better; untill there are too many threads and not enough lists. Theoretically, given N lists and N threads, all lists can be operated on parallely, thus increasing throughput by a near factor of N.

This does not appear to be true in the above curves. The key distinction is PARALLELISM. An N-way partitioned list (given N threads) can effectively increase throughput by a factor of N (also ideally provided kernal level threads) where this is not the case with a single list. As mentioned above, and N-Way partitioned list distributes workload that can be done in parallel, more imporantly reduces lock contention due to a partioned critical sectin, while a single list does nothing to minimize lock contention or spread out the critical section.